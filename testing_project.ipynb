{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs id                0\n",
      "target            0\n",
      "ps_ind_01         0\n",
      "ps_ind_02_cat     0\n",
      "ps_ind_03         0\n",
      "ps_ind_04_cat     0\n",
      "ps_ind_05_cat     0\n",
      "ps_ind_06_bin     0\n",
      "ps_ind_07_bin     0\n",
      "ps_ind_08_bin     0\n",
      "ps_ind_09_bin     0\n",
      "ps_ind_10_bin     0\n",
      "ps_ind_11_bin     0\n",
      "ps_ind_12_bin     0\n",
      "ps_ind_13_bin     0\n",
      "ps_ind_14         0\n",
      "ps_ind_15         0\n",
      "ps_ind_16_bin     0\n",
      "ps_ind_17_bin     0\n",
      "ps_ind_18_bin     0\n",
      "ps_reg_01         0\n",
      "ps_reg_02         0\n",
      "ps_reg_03         0\n",
      "ps_car_01_cat     0\n",
      "ps_car_02_cat     0\n",
      "ps_car_03_cat     0\n",
      "ps_car_04_cat     0\n",
      "ps_car_05_cat     0\n",
      "ps_car_06_cat     0\n",
      "ps_car_07_cat     0\n",
      "ps_car_08_cat     0\n",
      "ps_car_09_cat     0\n",
      "ps_car_10_cat     0\n",
      "ps_car_11_cat     0\n",
      "ps_car_11         0\n",
      "ps_car_12         0\n",
      "ps_car_13         0\n",
      "ps_car_14         0\n",
      "ps_car_15         0\n",
      "ps_calc_01        0\n",
      "ps_calc_02        0\n",
      "ps_calc_03        0\n",
      "ps_calc_04        0\n",
      "ps_calc_05        0\n",
      "ps_calc_06        0\n",
      "ps_calc_07        0\n",
      "ps_calc_08        0\n",
      "ps_calc_09        0\n",
      "ps_calc_10        0\n",
      "ps_calc_11        0\n",
      "ps_calc_12        0\n",
      "ps_calc_13        0\n",
      "ps_calc_14        0\n",
      "ps_calc_15_bin    0\n",
      "ps_calc_16_bin    0\n",
      "ps_calc_17_bin    0\n",
      "ps_calc_18_bin    0\n",
      "ps_calc_19_bin    0\n",
      "ps_calc_20_bin    0\n",
      "dtype: int64\n",
      "Values of target [0 1]\n",
      "Number of 1: 21694 Percentage of dataset: 3.64475178592 %\n"
     ]
    }
   ],
   "source": [
    "## Data Preparation\n",
    "dat = pd.read_csv(\"data/train.csv\")\n",
    "## With and without the -1, because it could be of significance.\n",
    "count_nan = len(dat) - dat.count()\n",
    "print(\"Number of NaNs\", count_nan)\n",
    "X = dat.loc[:, dat.columns != \"target\"]\n",
    "y = dat.loc[:, \"target\"]\n",
    "\n",
    "print(\"Values of target\", y.unique())\n",
    "print(\"Number of 1:\", sum(y == 1), \"Percentage of dataset:\", sum(y==1)/len(y)*100, \"%\")\n",
    "print(\"Number of 0:\", sum(y == 0), \"Percentage of dataset:\", sum(y==0)/len(y)*100, \"%\")\n",
    "X_no_neg = X\n",
    "y_no_neg = y\n",
    "cat = pd.DataFrame()\n",
    "bn = pd.DataFrame()\n",
    "norm = pd.DataFrame()\n",
    "ids = X['id']\n",
    "\n",
    "## We ignore the id column for this step\n",
    "for i in X.columns[1:]:\n",
    "    if \"cat\" in i:\n",
    "        # Transform Median\n",
    "        cat[i] = dat.loc[:, i]\n",
    "    elif \"bin\" in i:\n",
    "        # Transform\n",
    "        bn[i] = dat.loc[:, i]\n",
    "    else:\n",
    "        # Do something\n",
    "        norm[i] = dat.loc[:, i]\n",
    "for i in cat.columns:\n",
    "    tmp = 0\n",
    "    cat.loc[cat[i] == -1, i] = np.median(cat.loc[cat[i] != -1,i])\n",
    "    tmp = tmp + sum(cat[i] == -1)\n",
    "    \n",
    "print(\"Number of -1 in data:\", tmp)\n",
    "\n",
    "for i in bn.columns:\n",
    "    tmp = 0\n",
    "    tmp = tmp + sum(bn[i] == -1)\n",
    "    print(i, \":\", tmp)\n",
    "    \n",
    "print(\"There are no negatives in bin columns\")\n",
    "for feature in cat:\n",
    "    temp.append(pd.get_dummies(cat[feature]))\n",
    "\n",
    "new_cat = pd.DataFrame()\n",
    "for feature in temp:\n",
    "    new_cat = pd.concat([new_cat, feature], axis=1)\n",
    "pipe = Pipeline([\n",
    "       (\"remove_neg_ones\", Imputer(missing_values=-1, strategy=\"mean\")),\n",
    "        (\"z-scaling\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "scaled_norm = pd.DataFrame(pipe.fit_transform(norm), columns = norm.columns)\n",
    "X_final = pd.concat([bn, new_cat, scaled_norm], axis=1)\n",
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(X_final, y)\n",
    "features = clf.feature_importances_\n",
    "important_features = X_final[X_final.columns[np.argsort(-features)[:10]]]\n",
    "\n",
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "     assert( len(actual) == len(pred) )\n",
    "     all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "     all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "     totalLosses = all[:,0].sum()\n",
    "     giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "     giniSum -= (len(actual) + 1) / 2\n",
    "     return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X = pd.concat([bn, new_cat, scaled_norm], axis=1)\n",
    "\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=50, max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predicted)*100, \"%\")\n",
    "X_ones = scaled_X[y==1, :]\n",
    "y_ones = y[y==1]\n",
    "X_zeroes = scaled_X[y==0, :]\n",
    "dat_ones = pd.DataFrame(X_ones)\n",
    "dat_ones[\"target\"] = [1] * len(dat_ones)\n",
    "dat_f = pd.DataFrame(X_zeroes)\n",
    "dat_f[\"target\"] = np.zeros(len(dat_f))\n",
    "dat_ones = dat_ones.append(dat_f.sample(n=(len(dat_ones))))\n",
    "print(\"Number of 1 after rebuilding data:\", sum(dat_ones.target == 1), \"Percentage of dataset:\", sum(dat_ones.target==1)/len(dat_ones.target)*100, \"%\")\n",
    "print(\"Number of 0 after rebuilding data:\", sum(dat_ones.target == 0), \"Percentage of dataset:\", sum(dat_ones.target==0)/len(dat_ones.target)*100, \"%\")\n",
    "\n",
    "print(\"Reclassifing\")\n",
    "dat_ones.target\n",
    "X_shrunk = dat_ones.loc[:, dat_ones.columns != \"target\"]\n",
    "y_shrunk = dat_ones.target\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X_shrunk)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shrunk, y_shrunk)\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=50, max_depth=3)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_test)\n",
    "print(\"Confusion matrix\")\n",
    "print(confusion_matrix(y_test, predicted))\n",
    "print(\"Accuracy of new set:\", accuracy_score(y_test, predicted)*100, \"%\")\n",
    "print(\"Gini for Gradient Boosting:\", gini_normalized(y_test, predicted))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"Predicting from RandomForestClassifier\")\n",
    "rfc = RandomForestClassifier(n_estimators = 500)\n",
    "rfc.fit(X_train, y_train)\n",
    "predicted_rfc = rfc.predict(X_test)\n",
    "print(confusion_matrix(y_test, predicted_rfc))\n",
    "print(\"Accuracy of new set as RFC:\", accuracy_score(y_test, predicted_rfc)*100, \"%\")\n",
    "print(\"Gini for Random Forest:\", gini_normalized(y_test, predicted_rfc))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shrunk, y_shrunk, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "dtc = tree.DecisionTreeClassifier(max_depth=3)\n",
    "rndf = RandomForestClassifier()\n",
    "\n",
    "gnb.fit(X_train, y_train)\n",
    "gnbpred = gnb.predict(X_val)\n",
    "print (\"GNB Classifier Accuracy\", accuracy_score(y_val, gnbpred)*100, \"%\")\n",
    "print(\"Gini for GNB:\", gini_normalized(y_val, gnbpred))\n",
    "dtc = dtc.fit(X_train, y_train)\n",
    "dtcpred = dtc.predict(X_val)\n",
    "print (\"Decision Tree Classifier Accuracy\", accuracy_score(y_val, dtcpred)*100, \"%\")\n",
    "print(\"Gini for Decision Tree:\", gini_normalized(y_val, dtcpred))\n",
    "rndf.fit(X_train, y_train)\n",
    "rndfpred = rndf.predict(X_val)\n",
    "#print (\"Random Forest Accuracy\", accuracy_score(y_val, rndfpred)*100, \"%\")\n",
    "#print(\"Gini for Random Forest:\", gini_normalized(y_val, rndfpred))\n",
    "\n",
    "\n",
    "X_b = np.empty([len(y_val), 3])\n",
    "X_b[:, 0] = gnbpred\n",
    "X_b[:, 1] = dtcpred\n",
    "X_b[:, 2] = rndfpred\n",
    "y_b = y_val\n",
    "rfcA = RandomForestClassifier()\n",
    "rfcA.fit(X_b, y_b)\n",
    "\n",
    "pred = np.empty([len(y_test), 3])\n",
    "l = [gnb,dtc,rndf]\n",
    "tmp = 0\n",
    "for i in l:\n",
    "    pred[:,tmp] = i.predict(X_test)\n",
    "    tmp = tmp+1\n",
    "predictions = rfcA.predict(pred)\n",
    "print (\"MYvoting classifier Accuracy\", accuracy_score(y_test, predictions)*100, \"%\")\n",
    "print(\"Gini for MYvoting classifier:\", gini_normalized(y_test, predictions))\n",
    "\n",
    "vote = VotingClassifier(estimators=[('gnb', gnb),('rf', rndf), ('dtc', dtc)], voting='hard')\n",
    "vote.fit(X_train, y_train)\n",
    "votepred = vote.predict(X_test)\n",
    "print (\"\\nVoting Classifier\", accuracy_score(y_test, votepred)*100, \"%\")\n",
    "print(\"Gini for Voting Classifier:\", gini_normalized(y_test, votepred))\n",
    "\n",
    "class StackingRegressor:\n",
    "    def __init__(self, l_estimators):\n",
    "        self.l_estimators = l_estimators\n",
    "        self.models = []\n",
    "        self.blender = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Make a matrix of predictions\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        for i in self.l_estimators:\n",
    "            #bag_clf = BaggingRegressor(LogisticRegression(), n_estimators=50, max_samples=int(len(y)*.75), bootstrap=True, n_jobs=-1)\n",
    "            #bag_clf.fit(X, y)\n",
    "            model = i.fit(X_train,y_train)\n",
    "            self.models.append(model)\n",
    "        predictions = np.empty([len(y_test), len(self.l_estimators) + 1])\n",
    "        tmp = 0\n",
    "        for i in self.models:\n",
    "            predictions[:,tmp] = i.predict(X_test)\n",
    "            tmp = tmp + 1\n",
    "        predictions[:,len(self.l_estimators)] = y_test\n",
    "        rfcA = RandomForestClassifier()\n",
    "        Y = y_test\n",
    "        x = predictions[:,0:tmp]\n",
    "        rfcA.fit(x, Y)\n",
    "        self.blender = rfcA\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = np.empty([len(X), len(self.l_estimators)])\n",
    "        tmp = 0\n",
    "        for i in self.models:\n",
    "            predictions[:,tmp] = i.predict(X)\n",
    "            tmp = tmp + 1\n",
    "        return self.blender.predict(predictions)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        s = StackingRegressor(l_estimators = self.l_estimators)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "        s.fit(X_train,y_train)\n",
    "        pred = s.predict(X_test)\n",
    "        return accuracy_score(y_test, pred)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shrunk, y_shrunk)\n",
    "gnb = GaussianNB()\n",
    "dtc = tree.DecisionTreeClassifier(max_depth=3)\n",
    "rndf = RandomForestClassifier()\n",
    "stacker = StackingRegressor(l_estimators=[gnb,dtc,rndf])\n",
    "stacker.fit(X_train, y_train)\n",
    "predict = stacker.predict(X_test)\n",
    "\n",
    "print (\"Stacking Accuracy:\", accuracy_score(y_test, predict) * 100, \"%\")\n",
    "\n",
    "print(\"Gini for blender:\", gini_normalized(y_test, predict))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y)\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "confusion_matrix(y_test, np.round(predicted))\n",
    "\n",
    "print(f\"Gini score for default GradientBoostingRegressor {gini_normalized(y_test, predicted)}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shrunk, y_shrunk)\n",
    "\n",
    "clf = GradientBoostingRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "confusion_matrix(y_test, np.round(predicted))\n",
    "\n",
    "print(f\"Gini score for default GradientBoostingRegressor using shrunk dataset {gini_normalized(y_test, predicted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
