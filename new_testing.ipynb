{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"data/train.csv\")\n",
    "X_orig = dat.loc[:, dat.columns != \"target\"]\n",
    "y_orig = dat.loc[:, \"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_ind_06_bin : 0\n",
      "ps_ind_07_bin : 0\n",
      "ps_ind_08_bin : 0\n",
      "ps_ind_09_bin : 0\n",
      "ps_ind_10_bin : 0\n",
      "ps_ind_11_bin : 0\n",
      "ps_ind_12_bin : 0\n",
      "ps_ind_13_bin : 0\n",
      "ps_ind_16_bin : 0\n",
      "ps_ind_17_bin : 0\n",
      "ps_ind_18_bin : 0\n",
      "ps_calc_15_bin : 0\n",
      "ps_calc_16_bin : 0\n",
      "ps_calc_17_bin : 0\n",
      "ps_calc_18_bin : 0\n",
      "ps_calc_19_bin : 0\n",
      "ps_calc_20_bin : 0\n",
      "ps_ind_01 : 0\n",
      "ps_ind_03 : 0\n",
      "ps_ind_14 : 0\n",
      "ps_ind_15 : 0\n",
      "ps_reg_01 : 0\n",
      "ps_reg_02 : 0\n",
      "ps_reg_03 : 107772\n",
      "ps_car_11 : 5\n",
      "ps_car_12 : 1\n",
      "ps_car_13 : 0\n",
      "ps_car_14 : 42620\n",
      "ps_car_15 : 0\n",
      "ps_calc_01 : 0\n",
      "ps_calc_02 : 0\n",
      "ps_calc_03 : 0\n",
      "ps_calc_04 : 0\n",
      "ps_calc_05 : 0\n",
      "ps_calc_06 : 0\n",
      "ps_calc_07 : 0\n",
      "ps_calc_08 : 0\n",
      "ps_calc_09 : 0\n",
      "ps_calc_10 : 0\n",
      "ps_calc_11 : 0\n",
      "ps_calc_12 : 0\n",
      "ps_calc_13 : 0\n",
      "ps_calc_14 : 0\n",
      "~ 18.1064897885 % of the data is missing, so we are going to assume that there is significance that the data is missing\n"
     ]
    }
   ],
   "source": [
    "X_no_neg = X_orig\n",
    "y_no_neg = y_orig\n",
    "cat = pd.DataFrame()\n",
    "bn = pd.DataFrame()\n",
    "norm = pd.DataFrame()\n",
    "ids = X_orig['id']\n",
    "\n",
    "## We ignore the id column for this step\n",
    "for i in X_orig.columns[1:]:\n",
    "    if \"cat\" in i:\n",
    "        # Transform Median\n",
    "        cat[i] = dat.loc[:, i]\n",
    "    elif \"bin\" in i:\n",
    "        # Transform\n",
    "        bn[i] = dat.loc[:, i]\n",
    "    else:\n",
    "        # Do something\n",
    "        norm[i] = dat.loc[:, i]\n",
    "        \n",
    "for i in cat.columns:\n",
    "    tmp = 0\n",
    "    cat.loc[cat[i] == -1, i] = np.median(cat.loc[cat[i] != -1,i])\n",
    "    tmp = tmp + sum(cat[i] == -1)\n",
    "    \n",
    "#print(\"Number of -1 in data:\", tmp)\n",
    "\n",
    "for i in bn.columns:\n",
    "    tmp = 0\n",
    "    tmp = tmp + sum(bn[i] == -1)\n",
    "    print(i, \":\", tmp)\n",
    "    \n",
    "#print(\"There are no negatives in bin columns\")\n",
    "temp = []\n",
    "for feature in cat:\n",
    "    temp.append(pd.get_dummies(cat[feature]))\n",
    "\n",
    "new_cat = pd.DataFrame()\n",
    "for feature in temp:\n",
    "    new_cat = pd.concat([new_cat, feature], axis=1)\n",
    "\n",
    "mx = 0\n",
    "for i in norm.columns:\n",
    "    tmp = 0\n",
    "    tmp = tmp + sum(norm[i] == -1)\n",
    "    print(i, \":\", tmp)\n",
    "    if mx < tmp:\n",
    "        mx=tmp\n",
    "\n",
    "print(\"~\", mx/float(len(norm)) * 100, \"% of the data is missing, so we are going to assume that there is significance that the data is missing\")\n",
    "num_missing_per_row = np.zeros(len(norm))\n",
    "tmp = 0\n",
    "for index, row in norm.iterrows():\n",
    "    num_missing_per_row[index] =  sum(row == -1)\n",
    "    \n",
    "pipe = Pipeline([\n",
    "       (\"remove_neg_ones\", Imputer(missing_values=-1, strategy=\"mean\")),\n",
    "        (\"z-scaling\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "scaled_norm = pd.DataFrame(pipe.fit_transform(norm), columns = norm.columns)\n",
    "X_final = pd.concat([bn, new_cat, scaled_norm], axis=1)\n",
    "X_orig = X_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_orig, y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'pandas.core.frame.DataFrame'> Len 446409 Column len 218 orig column len 218\n",
      "Type: <class 'pandas.core.series.Series'> Len 446409\n"
     ]
    }
   ],
   "source": [
    "print(\"Type:\", type(X_train), \"Len\", len(X_train), \"Column len\", len(X_train.columns), \"orig column len\", \n",
    "      len(X_orig.columns))\n",
    "print(\"Type:\", type(y_train), \"Len\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boolList = np.asarray(y_orig==1)\n",
    "\n",
    "X_ones = X_train.loc[y_train==1, :]\n",
    "#idx = np.random.randint(len(X_ones) * 2, size=2)\n",
    "X_ones = pd.DataFrame(X_ones)\n",
    "X_ones = X_ones.sample(n=(len(X_ones)*4), replace=True)\n",
    "y_ones = y_train[y_train==1]\n",
    "X_zeroes = X_train.loc[y_train==0, :]\n",
    "X_zeroes = pd.DataFrame(X_zeroes)\n",
    "dat_ones = X_ones\n",
    "dat_ones[\"target\"] = [1] * (len(X_ones))\n",
    "dat_f = pd.DataFrame(X_zeroes.sample(n=(len(X_ones)), replace=True))\n",
    "dat_f[\"target\"] = np.zeros(len(dat_f))\n",
    "dat_ones = dat_ones.append(dat_f.sample(n=(len(dat_ones))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_ones = X_orig.loc[y_orig, :]\n",
    "#y_ones = y_orig[y_orig==1]\n",
    "#X_zeroes = X_orig.loc[y_orig==0, :]\n",
    "#dat_ones = pd.DataFrame(X_ones)\n",
    "#dat_ones[\"target\"] = [1] * len(dat_ones)\n",
    "#dat_f = pd.DataFrame(X_zeroes)\n",
    "#dat_f[\"target\"] = np.zeros(len(dat_f))\n",
    "#dat_ones = dat_ones.append(dat_f.sample(n=(len(dat_ones)), replace=True))\n",
    "#X_shrunk = dat_ones.loc[:, dat_ones.columns != \"target\"]\n",
    "#y_shrunk = dat_ones.target\n",
    "\n",
    "\n",
    "\n",
    "#dat_ones.target.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shrunk = dat_ones.loc[:, dat_ones.columns != \"target\"]\n",
    "y_shrunk = dat_ones.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gini(actual, pred, cmpcol = 0, sortcol = 1):\n",
    "     assert( len(actual) == len(pred) )\n",
    "     all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n",
    "     all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n",
    "     totalLosses = all[:,0].sum()\n",
    "     giniSum = all[:,0].cumsum().sum() / totalLosses\n",
    " \n",
    "     giniSum -= (len(actual) + 1) / 2\n",
    "     return giniSum / len(actual)\n",
    " \n",
    "def gini_normalized(a, p):\n",
    "    return gini(a, p) / gini(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict_proba(X_test)\n",
    "#confusion_matrix(y_test, np.round(predicted))\n",
    "#print(np.amin(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini score for default GradientBoostingRegressor 0.28252667475253185\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gini score for default GradientBoostingRegressor {gini_normalized(y_test, predicted[:,1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference of splits are large (.004 in change)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
